{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8895dcbc-5fe1-4966-b226-1db5bcc54452",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Baseline llama2 performance\n",
    "- Running on g5.4xlarge instance\n",
    "- Treating \"baseline\" as loading model in `bfloat16` and using `device_map=\"auto\"` (running on CPU or with full-precision are both very, very slow. For reference, model loaded in `torch.float32` takes 18.5GB CUDA memory compared to 12.6GB with `torch.bfloat16`; generates 1.3 tokens per second compared to ~28 tokens per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4efbd752-0270-456e-bf20-89089900b424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade torch transformers accelerate huggingface_hub\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dca21ce-0dec-4d57-8013-ac2e42490021",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils import generate_text, clear_model, torch_profile_to_dataframe\n",
    "import huggingface_hub\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import accelerate\n",
    "\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Dreams are\",\n",
    "    \"The future of technology is\",\n",
    "    \"In a world where magic exists,\",\n",
    "    \"The most influential person in history is\",\n",
    "    \"One of the most intriguing mysteries of the universe is\",\n",
    "    \"When humans finally ventured out into the cosmos, they discovered\",\n",
    "    \"The relationship between artificial intelligence and humanity has always been\",\n",
    "    \"As the boundaries of science and fiction blur, the implications for society become\",\n",
    "    \"In the depths of the enchanted forest, ancient creatures and forgotten tales come to life, revealing\",\n",
    "    \"While many believe that technological advancements will be the key to solving humanity's greatest challenges, others argue that it will only exacerbate existing inequalities, leading to\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f3340e0-67c6-43e5-b2b5-f18cfb0bc32e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81cf0b17-b81e-4615-af81-bf5c036a7519",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", use_cache=True, padding_side=\"left\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d868f22-5f30-4cea-92ea-03297e82d9bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e7939ed-e6d2-43ab-bfee-343c7a19d374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5175dbf-36f2-4170-b80d-00a66c06efbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Throughput and Memory\n",
    "\n",
    "## Serial Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e983609-6d56-4bda-b471-488864ed9303",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=False,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f49f53f-5978-4399-9ab2-d0bf00daa379",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Batch prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e5a838-2d54-4426-9c3c-e0b52f378492",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=True,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ba79f06-ee39-4690-a90f-83ba3faf031d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Torch Profiling -- Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac0cb2f3-a407-4d15-a076-bfb0c35d7fbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "with profiler.profile(\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    "    experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True),\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    ") as prof:\n",
    "  output = generate_text(prompts, model, tokenizer, eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_new_tokens=10)\n",
    "\n",
    "torch_profile_to_dataframe(prof).sort_values(\"Self CUDA %\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ce9ebd-c3ed-4c6d-b4f8-28de6e5870b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Visualize the PyTorch Trace (single token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7b5a173-aa41-4087-a07f-603029cb4ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "with profiler.profile(\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True,\n",
    "    #on_trace_ready=torch.profiler.tensorboard_trace_handler('./tmp/log/llamalog'),\n",
    "    experimental_config=torch._C._profiler._ExperimentalConfig(verbose=True),\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    ") as prof:\n",
    "  output = generate_text(\"The greatest innovations in technology in the 21st century were\", model, tokenizer,\n",
    "                         eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bb78c1c-1e71-4ed7-9e80-2dcc97967cf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"/dbfs/<path>/baseline_trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd4d7def-9407-4f8c-8f67-b797bb84bb4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can visualize the trace by following these steps:\n",
    "1. Download the trace json file to the local machine. From the command line on the local machine: `databricks fs cp dbfs:<path>/trace.json ~/Downloads/baseline_trace.json`\n",
    "2. Open the trace file with Google Chrome via `chrome://tracing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b280db0-93bd-4107-8d5b-cb7cd1d05832",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1. baseline",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
