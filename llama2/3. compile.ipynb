{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "710bbe8e-c273-408b-ad3a-e1780fcdb08f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# torch.compile\n",
    "\n",
    "Using [`torch.compile()`](https://huggingface.co/docs/transformers/perf_torch_compile). Compare to the baseline performance [here](https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#notebook/418210139975057).\n",
    "\n",
    "Appears to result in little to no improvement in tokens per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0c41b3-cbf7-4e42-8a77-d56bf66f906e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade torch transformers accelerate huggingface_hub\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f512ae2c-ac9c-4712-9fa5-a715230df126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils import generate_text, clear_model, torch_profile_to_dataframe\n",
    "import huggingface_hub\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import accelerate\n",
    "\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Dreams are\",\n",
    "    \"The future of technology is\",\n",
    "    \"In a world where magic exists,\",\n",
    "    \"The most influential person in history is\",\n",
    "    \"One of the most intriguing mysteries of the universe is\",\n",
    "    \"When humans finally ventured out into the cosmos, they discovered\",\n",
    "    \"The relationship between artificial intelligence and humanity has always been\",\n",
    "    \"As the boundaries of science and fiction blur, the implications for society become\",\n",
    "    \"In the depths of the enchanted forest, ancient creatures and forgotten tales come to life, revealing\",\n",
    "    \"While many believe that technological advancements will be the key to solving humanity's greatest challenges, others argue that it will only exacerbate existing inequalities, leading to\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5769a68f-ad7f-4bf2-994b-3bef33c55cdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6474a0-8d79-4e02-bf0f-419d738f209f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", use_cache=True, padding_side=\"left\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model=torch.compile(model, mode=\"max-autotune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2898eac0-b0dd-4185-8ef6-b100834b4df5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0574027d-6a38-4c29-9349-3f24d7627a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7dfef6a-c62e-4539-9df6-3ded21b8239a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Throughput and Memory\n",
    "\n",
    "## Serial Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3e236f-baca-467c-b450-6fd03b0fdfb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=False,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22a4f04-c878-417d-8f7e-835326a33d76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Batch prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008b2ab5-291b-4a92-8a4a-ebdddbfe492b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=True,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bcd95b1-8c8d-4c69-a46d-27e0414b3f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Torch Profiling -- Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5020a94-2f2c-4eac-8564-088483ba1523",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "with profiler.profile(\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    ") as prof:\n",
    "  output = generate_text(prompts, model, tokenizer, eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_new_tokens=10)\n",
    "\n",
    "torch_profile_to_dataframe(prof).sort_values(\"Self CUDA %\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3. compile",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
