{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5927fa03-6d37-480b-bf82-872a92c2d68f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# model.to_bettertransformer()\n",
    "\n",
    "Using [`model.to_bettertransformer()`](https://huggingface.co/docs/transformers/perf_infer_gpu_one#decoder-models). Compare to the baseline performance [here](https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#notebook/418210139975057)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19933353-a6f5-4916-bedf-32f65adca39d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade torch transformers accelerate huggingface_hub optimum\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f75db7f-69da-41f5-95b0-e869a7f1ce6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils import generate_text, clear_model, torch_profile_to_dataframe, wrap_module_with_profiler\n",
    "import huggingface_hub\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import accelerate\n",
    "\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Dreams are\",\n",
    "    \"The future of technology is\",\n",
    "    \"In a world where magic exists,\",\n",
    "    \"The most influential person in history is\",\n",
    "    \"One of the most intriguing mysteries of the universe is\",\n",
    "    \"When humans finally ventured out into the cosmos, they discovered\",\n",
    "    \"The relationship between artificial intelligence and humanity has always been\",\n",
    "    \"As the boundaries of science and fiction blur, the implications for society become\",\n",
    "    \"In the depths of the enchanted forest, ancient creatures and forgotten tales come to life, revealing\",\n",
    "    \"While many believe that technological advancements will be the key to solving humanity's greatest challenges, others argue that it will only exacerbate existing inequalities, leading to\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f923954d-f3f1-44d5-ab6e-5c5236ef3eef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e82ba9ee-6456-48ea-8ff0-f3794c87cbf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", use_cache=True, padding_side=\"left\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    use_cache=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "130b5dc7-0a27-49c9-94f7-5416ae4b9eef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28407176-c678-4264-9fd1-e52beb70395c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93cbbabe-eb90-4f0b-9a14-b06241d92010",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Throughput and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaae0b03-d9c8-4dbe-8e58-13615a059c22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=False,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "002b15ad-5b59-4372-a970-118bd9881842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e99eacf6-be43-4fc2-b55d-3071d5ac8cd9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Torch Profiling -- Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0243315c-9461-4954-a6c1-87ad78c313ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "with profiler.profile(\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    ") as prof:\n",
    "  output = generate_text(prompts, model, tokenizer, eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_new_tokens=10)\n",
    "\n",
    "torch_profile_to_dataframe(prof).sort_values(\"Self CUDA %\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2. better_transformer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
