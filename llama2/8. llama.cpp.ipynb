{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "966a9aa7-7096-422a-b02c-5d756c02a43c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook shows how to use llama.cpp via the [lama-cpp-python](https://github.com/abetlen/llama-cpp-python) package.\n",
    "\n",
    "llama.cpp enables model inference in c/c++. Its original goal was to \"run the LLaMA model using 4-bit integer quantization on a MacBook\" but its scope has since expanded considerably.\n",
    "\n",
    "Here, we will use it with CUDA.\n",
    "\n",
    "# 1. Install llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8657031-f1bb-4f97-b422-7616c4e99950",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/ggerganov/llama.cpp /databricks/driver/llama.cpp/\n",
    "%cd /databricks/driver/llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82d0f25e-483d-4fc9-ade0-c41baf91ffec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install nvidia cuda toolkit\n",
    "!apt-get install nvidia-cuda-toolkit -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1b10d3d-3147-4601-adc1-e0c99989a971",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Build llama.cpp with cuBLAS support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db768cc-1e87-4b6b-9541-66d67b77ded8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir build\n",
    "cd build\n",
    "cmake .. -DLLAMA_CUBLAS=ON\n",
    "cmake --build . --config Release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64dd29a7-4be6-4489-8edd-1640952c36c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Skip to step 5 if you've already converted and quantized the model and saved the quantized model!\n",
    "# 2. Download or move the model to the ./models directory of llama.cpp\n",
    "First, download the model from [Hugging Face](https://huggingface.co/meta-llama/Llama-2-7b-hf). You can use, e.g.,\n",
    "\n",
    "```\n",
    "git lfs install\n",
    "git clone git@hf.co:meta-llama/Llama-2-7b-hf <target_directory>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1098644d-ab36-4108-808d-22da7a5e5493",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%cp -r /dbfs/daniel.liden/models/llama2/ ./models/llama2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d04bbad1-aaf9-42c6-9ab9-9fe63cc1fd00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install python dependencies\n",
    "!python3 -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0da43995-020f-4f2f-a132-57cacc832146",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Convert the model to the gguf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4844eef7-1317-47f8-b0a4-c36b7e8d7020",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!python3 convert.py ./models/llama2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a107f19a-bcde-405a-aeac-9548a4ef30cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# verify we now have a gguf model\n",
    "!find ./models/llama2/ -name \"*.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "168fb939-7611-4fc4-8280-4a6e1921483e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Quantize the model\n",
    "\n",
    "You can see the different quantization options with `quantize --help`. Recommendations can be found in [this issue](https://github.com/ggerganov/llama.cpp/discussions/2094) (may not be up to date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5f7de28-de88-4d91-b35d-25f2ef94940e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!./build/bin/quantize --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a06bc58c-0c7f-4195-a5a6-6838cf2bc12a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!./build/bin/quantize ./models/llama2/ggml-model-f16.gguf ./models/llama2/ggml-model-q5_k_m.gguf q5_k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ff9a27f-5af5-4ae5-916c-c59f34fb47e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Optionally, save the quantized model to dbfs\n",
    "import os\n",
    "source_path = \"./models/llama2/ggml-model-q5_k_m.gguf\"\n",
    "target_path = \"/daniel.liden/models/ggml-model-q5_k_m.gguf\"\n",
    "\n",
    "dbutils.fs.cp(\"file:\" + os.path.abspath(source_path), target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c096d8-fe3c-4c83-969f-e48dd1e9b632",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Run inference\n",
    "\n",
    "Resume here if you've already downloaded and quantized the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f348125c-0bda-40c1-9751-7b54a769ef42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"daniel.liden/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fcc7218-1bc4-4a92-8604-0e217e0819f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%cp /dbfs/daniel.liden/models/ggml-model-q5_k_m.gguf ./models/ggml-model-q5_k_m.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a68a9ee3-fcac-41bb-852d-1d0c2538a2fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!ls ./models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e814c36-365a-449b-b1bb-e685bee78931",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run the inference\n",
    "!./build/bin/main -m ./models/ggml-model-q5_k_m.gguf -n 128 -ngl 64 --prompt \"The steps to make a good chemex pour-over coffee are as follows:\\n1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d473f29-86eb-4eac-a627-5d8c364aeff3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 6. Python bindings with [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\n",
    "\n",
    "Once you have a gguf-formatted and quantized model, you can use the high-level Python API offered by `llama-cpp-python` to work with it. See notebook 8a for details."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "8. llama.cpp",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
