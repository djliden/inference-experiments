{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bc498a3-0fa5-48af-bb49-3129fe390a21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Quantize models with GPTQ\n",
    "\n",
    "Using the [`AutoGPTQ` library](https://github.com/PanQiWei/AutoGPTQ#quick-installation). Compare to the baseline performance [here](https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#notebook/418210139975057).\n",
    "\n",
    "This approach resulted in a model memory footprint of 3.9GB and approximately 32 tokens per second throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4a36c8-c29f-48e8-93ac-98b228ae582d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notes and Resources\n",
    "\n",
    "**Notes**\n",
    "As of 2023-08-28, there were still some gotchas and sharp edges with trying to use AutoGPTQ. In particular, following the examples in the [Making LLMs lighter with AutoGPTQ and transformers](https://huggingface.co/blog/gptq-integration#native-support-of-gptq-models-in-%F0%9F%A4%97-transformers) release blog resulted in CUDA out-of-memory errors. To fix this, it was necessary to remove the `device_map=\"auto\"` argument. According to a [recent issue](https://github.com/PanQiWei/AutoGPTQ/issues/291#issuecomment-1695646845) in the AutoGPTQ library, AutoGPTQ automatically uses GPUs correctly for quantization; it appears there are some undesirable interactions with the device mapping from the accelerate library.\n",
    "\n",
    "**Docs and Further Reading**\n",
    "- https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py\n",
    "- https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization\n",
    "- https://huggingface.co/docs/transformers/main_classes/quantization\n",
    "- https://huggingface.co/docs/transformers/v4.32.1/en/main_classes/quantization#transformers.GPTQConfig\n",
    "- https://github.com/PanQiWei/AutoGPTQ/issues/179\n",
    "- https://github.com/PanQiWei/AutoGPTQ/issues/291\n",
    "- https://github.com/PanQiWei/AutoGPTQ/issues/291#issuecomment-1695992421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e2b0f35-384a-4b99-824a-f53b4ac9de3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade torch transformers accelerate huggingface_hub optimum\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7504161f-4a8a-448b-9937-088152858eb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install the AutoGPTQ Library corresponding to the CUDA version (11.8)\n",
    "%pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c321b2c7-5036-4c6a-8628-910d45d98bf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils import generate_text, clear_model, torch_profile_to_dataframe, wrap_module_with_profiler\n",
    "import huggingface_hub\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20933bb6-0bdd-4d18-9af2-9789ee74751c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "016542f6-b8b9-4c38-afaf-2b04c493a4c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, GPTQConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", use_cache=True, padding_side=\"left\"\n",
    ")\n",
    "\n",
    "quantization_config = GPTQConfig(\n",
    "    bits=4,\n",
    "    dataset=\"c4\",\n",
    "    tokenizer=tokenizer,\n",
    "    group_size=128,  # default\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51513415-433f-4e20-a9de-5a89e329b652",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "536f3e16-0432-475d-a52c-deae6a39abf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc1cc19-7449-47fa-80ca-1982ae53ae64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Save the model\n",
    "This will also save the quantization config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b37c0e5-fa35-44a1-9ede-037ba3ed628f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_folder = \"/dbfs/daniel.liden/models/llama2GPTQc4/\"\n",
    "model.save_pretrained(save_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0d600b5-cde3-49cc-bef9-2b1995e1e362",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load the quantized model\n",
    "First detach and reattach compute and then reinstall the required libraries. This is to make sure we accurately measure the CUDA memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "096c42e9-697f-43ff-a8d0-f5b0e72d50be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade torch transformers accelerate huggingface_hub optimum\n",
    "%pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97e06ae1-ffe1-4e34-9cf5-d0dd3783ff7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils import generate_text, clear_model, torch_profile_to_dataframe\n",
    "import huggingface_hub\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, pipeline, GPTQConfig\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import accelerate\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Dreams are\",\n",
    "    \"The future of technology is\",\n",
    "    \"In a world where magic exists,\",\n",
    "    \"The most influential person in history is\",\n",
    "    \"One of the most intriguing mysteries of the universe is\",\n",
    "    \"When humans finally ventured out into the cosmos, they discovered\",\n",
    "    \"The relationship between artificial intelligence and humanity has always been\",\n",
    "    \"As the boundaries of science and fiction blur, the implications for society become\",\n",
    "    \"In the depths of the enchanted forest, ancient creatures and forgotten tales come to life, revealing\",\n",
    "    \"While many believe that technological advancements will be the key to solving humanity's greatest challenges, others argue that it will only exacerbate existing inequalities, leading to\"\n",
    "]\n",
    "\n",
    "huggingface_hub.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd813576-efd8-495b-bda2-23d0c7253997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_folder = \"/dbfs/daniel.liden/models/llama2GPTQc4/\"\n",
    "gptq_config = GPTQConfig(bits=4, disable_exllama=False, use_cuda_fp16=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_folder,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=gptq_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", use_cache=True, padding_side=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1b918a7-2162-4156-a102-a69818c35a8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Throughput and Memory\n",
    "\n",
    "## Serial Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef94dd84-e20a-4728-870d-a472decc1d28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=False,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a1ac41-5217-41ab-ad7b-d7d569adf656",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Batch prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f426e22-a4e9-4597-89c8-4c12043d475a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=True,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "102fb653-06f1-4d45-a305-08fbe3aa304f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Torch Profiling -- Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79efa2b5-b1f0-4e42-acc4-70932616c973",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "with profiler.profile(\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    ") as prof:\n",
    "  output = generate_text(prompts, model, tokenizer, eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_new_tokens=10)\n",
    "\n",
    "torch_profile_to_dataframe(prof).sort_values(\"Self CUDA %\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "6. AutoGPTQ",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
