{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c74dc848-f0f9-470f-a1c3-313a3e1bf424",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# fp4 mixed-precision inference\n",
    "\n",
    "Using [`load_in_4bit=True`](https://huggingface.co/docs/transformers/perf_infer_gpu_one#running-fp4-models-single-gpu-setup-quickstart). Compare to the baseline performance [here](https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#notebook/418210139975057).\n",
    "\n",
    "- Low memory usage; reduced speed compared to baseline but faster than int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "746ed95a-7c3d-4e4f-b7b0-cbe7602c8b23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade torch transformers accelerate huggingface_hub bitsandbytes\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be5bca4c-d83d-4617-89ff-10a8efca7d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils import generate_text, clear_model, torch_profile_to_dataframe, wrap_module_with_profiler\n",
    "import huggingface_hub\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import accelerate\n",
    "\n",
    "\n",
    "\n",
    "prompts = [\n",
    "    \"Dreams are\",\n",
    "    \"The future of technology is\",\n",
    "    \"In a world where magic exists,\",\n",
    "    \"The most influential person in history is\",\n",
    "    \"One of the most intriguing mysteries of the universe is\",\n",
    "    \"When humans finally ventured out into the cosmos, they discovered\",\n",
    "    \"The relationship between artificial intelligence and humanity has always been\",\n",
    "    \"As the boundaries of science and fiction blur, the implications for society become\",\n",
    "    \"In the depths of the enchanted forest, ancient creatures and forgotten tales come to life, revealing\",\n",
    "    \"While many believe that technological advancements will be the key to solving humanity's greatest challenges, others argue that it will only exacerbate existing inequalities, leading to\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64ee9ed-5da9-4f78-976d-c009579d0dfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a806f9b-4714-4ca3-85f4-07f3de8e02a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", use_cache=True, padding_side=\"left\"\n",
    ")\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=False,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=nf4_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f643886c-2aac-4014-bf8c-d9cef2316332",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inspect Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b737831-114d-4670-958c-cb99a6b9b734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60e6287e-353f-4d33-8796-1525b661c1e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Throughput and Memory\n",
    "\n",
    "## Serial inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba4fb416-f17b-422e-ba92-9326a73160e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=False,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "173b2f8e-c1bc-43ec-94eb-42189f1a1b64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3892057-47af-439e-bc23-fe07ff6328b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text(prompts, model, tokenizer, batch=True,\n",
    "              eos_token_id=tokenizer.eos_token_id, max_new_tokens=50)\n",
    "pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d757fbc-0fa8-4911-844a-db342498fecf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Torch Profiling -- Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55200f78-2941-4778-b251-7b4fd17871aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "with profiler.profile(\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    ") as prof:\n",
    "  output = generate_text(prompts, model, tokenizer, eos_token_id=tokenizer.eos_token_id,\n",
    "                         max_new_tokens=10)\n",
    "\n",
    "torch_profile_to_dataframe(prof).sort_values(\"Self CUDA %\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "5. fp4 mixed-precision inference",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
