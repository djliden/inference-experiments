{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd1e143-26c3-4b08-aae8-a69220eb2f18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Python bindings with [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\n",
    "\n",
    "Once you have a gguf-formatted and quantized model, you can use the high-level Python API offered by `llama-cpp-python` to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e58e6f-4269-4b41-b7e0-30cac218a3d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%cd /databricks/driver/\n",
    "%cp /dbfs/daniel.liden/models/ggml-model-q5_k_m.gguf ./ggml-model-q5_k_m.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "250a1546-a7bb-4b98-9d7d-12887fd2f2a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# install nvidia cuda toolkit\n",
    "!apt-get install nvidia-cuda-toolkit -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4a95664-7074-47ff-9ffa-daf5ae7c35ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc51c7f7-f547-4b93-abb3-fccae25f1826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "# make sure to set `n_gpu_layers` to use GPU.\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"./ggml-model-q5_k_m.gguf\", n_gpu_layers = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "788ae168-d9f5-423e-9b99-cc30e1ab6ce9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output = llm(\"The steps to make a good chemex pour-over coffee are as follows:\\n1.\", max_tokens=250, echo=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "725d8259-e919-46c0-836b-4a37da3a9454",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Throughput and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18d1890c-7368-45e9-98cd-a1aeb4736042",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils import generate_text_llama_cpp_py, torch_profile_to_dataframe\n",
    "import pandas as pd\n",
    "\n",
    "prompts = [\n",
    "    \"Dreams are\",\n",
    "    \"The future of technology is\",\n",
    "    \"In a world where magic exists,\",\n",
    "    \"The most influential person in history is\",\n",
    "    \"One of the most intriguing mysteries of the universe is\",\n",
    "    \"When humans finally ventured out into the cosmos, they discovered\",\n",
    "    \"The relationship between artificial intelligence and humanity has always been\",\n",
    "    \"As the boundaries of science and fiction blur, the implications for society become\",\n",
    "    \"In the depths of the enchanted forest, ancient creatures and forgotten tales come to life, revealing\",\n",
    "    \"While many believe that technological advancements will be the key to solving humanity's greatest challenges, others argue that it will only exacerbate existing inequalities, leading to\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "324db3c9-74df-45f6-896c-4e69534a60fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out = generate_text_llama_cpp_py(prompts, model=llm, batch=False,\n",
    "              max_tokens=50)\n",
    "pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a890085a-e0ad-4426-a289-8e9f79d4fd38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Torch Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6b5f64a-49af-499a-a153-0206a0abb22a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "\n",
    "with profiler.profile(\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    ") as prof:\n",
    "  output = generate_text_llama_cpp_py(prompts, model=llm, batch=False,\n",
    "              max_tokens=50)\n",
    "\n",
    "torch_profile_to_dataframe(prof).sort_values(\"Self CUDA %\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf2dcde-085e-49ef-b79d-a929e6f77636",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "8a. llama-cpp-python",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
